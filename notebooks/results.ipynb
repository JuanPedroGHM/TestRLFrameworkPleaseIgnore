{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.chdir('..')\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from trlfpi.report import Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importExperimentData(experiment: str):\n",
    "    path = pathlib.Path(f\"results/{experiment}\")\n",
    "    \n",
    "    data = []\n",
    "    for x in path.iterdir():\n",
    "        report = Report(experiment, x.parts[-1])\n",
    "        variables = report.unpickle('variables')\n",
    "        evalResults = report.unpickle('evalResults')\n",
    "        args = report.getArgs()\n",
    "        data.append((args, variables, evalResults))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "def plotExpData(data, title):\n",
    "\n",
    "    rewardData = np.vstack(list(map(lambda x: x[1]['rewards'][0], data)))\n",
    "    \n",
    "    akey = 'actorLoss' if 'actorLoss' in data[0][1] else 'actor_loss'\n",
    "    actorLoss = np.vstack(list(map(lambda x: x[1][akey][0], data)))\n",
    "\n",
    "    ckey = 'criticLoss' if 'criticLoss' in data[0][1] else None\n",
    "    ckey = 'critic_loss' if 'critic_loss' in data[0][1] else None\n",
    "    if ckey:\n",
    "        criticLoss = np.vstack(list(map(lambda x: x[1][ckey][0], data)))\n",
    "    \n",
    "    if ckey:\n",
    "        fig, axs = plt.subplots(4, 1, sharex=True)\n",
    "        axs[3].plot(criticLoss.T)\n",
    "        axs[3].set_ylabel('loss')\n",
    "        axs[3].set_ylim((-10, 50000))\n",
    "        axs[3].set_title('Critic loss')\n",
    "        axs[3].grid()\n",
    "    else:\n",
    "        fig, axs = plt.subplots(3, 1, sharex=True)\n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    axs[0].plot(rewardData.T)\n",
    "    axs[0].set_ylabel('reward')\n",
    "    axs[0].set_ylim((-10000, 10))\n",
    "    axs[0].set_title('Experiment reward')\n",
    "    axs[0].grid()\n",
    "    \n",
    "    axs[1].plot(np.mean(rewardData, axis=0))\n",
    "    axs[1].set_ylabel('mean reward')\n",
    "    axs[1].set_title('Mean experiment reward')\n",
    "    axs[1].grid()\n",
    "    \n",
    "    axs[2].plot(actorLoss.T)\n",
    "    axs[2].set_ylabel('loss')\n",
    "    axs[2].set_title('Actor loss')\n",
    "    axs[2].grid()\n",
    "    \n",
    "    fig.show()\n",
    "    return np.mean(rewardData, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultsTable(name, data):\n",
    "    rewardData = np.array(list(map(lambda x: x[2]['meanReward'], data)))\n",
    "    rewardStats = {\n",
    "        'name' : name,\n",
    "        'mean' : rewardData.mean(),\n",
    "        'std' : rewardData.std(),\n",
    "        'best' : rewardData.max(),\n",
    "        'under1' : np.sum(rewardData > -1),\n",
    "        'under5' : np.sum(rewardData > -5)\n",
    "    }\n",
    "    print(f\"{name} mean reward : {rewardStats['mean']}\")\n",
    "    print(f\"{name} std reward : {rewardStats['std']}\")\n",
    "    print(f\"{name} best reward : {rewardStats['best']}\")\n",
    "    print(f\"{name} # under 1: {rewardStats['under1']}\")\n",
    "    print(f\"{name} # under 5: {rewardStats['under5']}\")\n",
    "    return rewardStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "def displayPDF(path: str):\n",
    "    display(IFrame(path, width=100, height=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env\n",
    "\n",
    "For testing, a simple linear environment with a randomly generated reference during each episode. While using always using the same reference might be more adecuate/better fitting for the ultimate goal, I belive that by doing this, the policy will actually learn the system dynamics instead of blindly repeating the same every time.\n",
    "\n",
    "$$ \\underline x_{k+1} =  A \\cdot \\underline x_k + B \\cdot \\underline u_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforce with baseline\n",
    "\n",
    "Implentation of the reinforce algorithm with unbiased rewards, on-policy, experience replay and importance sampling.\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\sum^{\\mathcal D}  \\omega \\cdot (c^{(i)} - \\overline c) \\cdot \\nabla_\\theta \\ln \\pi_\\theta(u^{(i)} | x^{(i)}, r^{(i)})  $$\n",
    "$$\\omega = \\frac{\\pi_\\theta(u^{(i)} | x^{(i)}, r^{(i)})}{\\pi'_\\theta(u^{(i)} | x^{(i)}, r^{(i)})}$$\n",
    "\n",
    "Baseline is calculated by calculated the mean cost in the batch $\\overline c = \\frac{1}{|\\mathcal D|} \\sum^{\\mathcal D} c_i$. At the time of sampling, the probability $\\pi'$ of the action is saved to weight its importance for the current policy update. \n",
    "\n",
    "The actor network is updated every timestep. The target network is used for generating samples from the env, and at the end of every episode, it copies the parameters from the actor network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "**Pro**\n",
    " - Only one approximator (Policy) (less hyperparams, less resorce intensive)\n",
    " - Able to learn on continuos action environments (All policy gradient algorithms)\n",
    " - Easy to apply on and off policy\n",
    " - It doesn't depend on the quality of another model to learn properly\n",
    "\n",
    "**Cons**\n",
    " - Unstable learning progress, not guaranteed to converge\n",
    " - Has to save full trajectories to use discounted rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforceData = importExperimentData('REINFORCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f776e23ffba84913a386975af97415cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reinforceMean = plotExpData(reinforceData, 'REINFORCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REINFORCE mean reward : -455.7885092826657\n",
      "REINFORCE std reward : 1156.09373007422\n",
      "REINFORCE best reward : -0.22156307652214458\n",
      "REINFORCE # under 1: 2\n",
      "REINFORCE # under 5: 3\n"
     ]
    }
   ],
   "source": [
    "reinforceStats = resultsTable('REINFORCE', reinforceData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic\n",
    "\n",
    "Same principle as the policy gradient, but the advantage term uses an approximation of the Q and Value function to give a better estimate of the expected reward given an action. Q-Function critic is trained using temporal differences error. Like for REINFORCE, a replay buffer and a target network are used to maintain stability during training. \n",
    "\n",
    "$$ C(\\phi) = \\sum^{\\mathcal D} \\left( c^{(i)}_t + \\gamma Q_{\\phi'}(u_{t+1}, x_{t+1}^{(i)}, r^{(i)}) - Q_{\\phi}(u_t^{(i)}, x_t^{(i)}, r^{(i)}) \\right)^2 |_{u_{t+1} = \\pi'} $$\n",
    "\n",
    "The updates are done a every step, just like with reinforce. The gradient of the policy function is updated to this.\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\sum^{\\mathcal D} \\omega \\cdot  A \\cdot \\nabla_\\theta \\ln \\pi_\\theta(u^{(i)} | x^{(i)}, r^{(i)}) $$\n",
    "\n",
    "$$ A = Q_\\phi (u^{(i)}, x^{(i)}, r^{(i)}) - V(x^{(i)}, r^{(i)}) $$\n",
    "\n",
    "The value function is calculated by sampling many (200) actions from the current policy and calculating the mean from the resulting Q values.\n",
    "To update the target networks, instead of a hard copy like in reinforce, a soft update is used every 2 steps. \n",
    "\n",
    "$$ \\theta' = (1 - \\tau) \\cdot \\theta' + \\tau \\cdot \\theta $$\n",
    "\n",
    "$\\tau$ is set to 0.001 for all experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "**Pro**\n",
    " - Provides a more stable learning process thanks to the Q-Function\n",
    " - Q-Function automaticly learns discounted rewards TD Error, providing it to the policy without saving full (or partial trajectories)\n",
    " - Provides (with enought time) better results than REINFORCE\n",
    "\n",
    "**Cons**\n",
    " - Slow learning to guarantee stability of Q-Function during training\n",
    " - Policy only learns properly once (and if) the critic has learnt something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93db8df44539440498a06d34844e2c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acData = importExperimentData('AC')\n",
    "acMean = plotExpData(acData, 'AC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AC mean reward : -232.3075069790143\n",
      "AC std reward : 558.8212808145927\n",
      "AC best reward : -25.013295636302747\n",
      "AC # under 1: 0\n",
      "AC # under 5: 0\n"
     ]
    }
   ],
   "source": [
    "acStats = resultsTable('AC', acData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPG\n",
    "\n",
    "DPG uses a deterministic policy, and it tries to update it by directly maximizing the Q-Function return.\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = \\sum^{\\mathcal D} \\nabla_a Q_\\phi(u, x^{(i)}, r^{(i)}) \\nabla_\\theta \\mu_\\theta(x^{(i)}, r^{(i)}) |_{u = \\mu_\\theta} $$\n",
    "\n",
    "Again, the implementation uses target networks and replay buffers. The main difference is that exploration is done by applying noise to the policy during sampling in the form of $u_t = \\mu(x_t, r_t) + \\sigma \\cdot \\mathcal N(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisys\n",
    "**Pros**\n",
    " - For now, best performing algorithm\n",
    " - Simpler policy (doesn't require variance params)\n",
    " - Fastest convergence\n",
    " \n",
    "**Cons**\n",
    "\n",
    " - Only allows deterministic policies ( exploration policy is needed in order to explore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb18a4e810514a13aae13e4ec3388d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpgData = importExperimentData('DPG')\n",
    "dpgMean = plotExpData(dpgData, 'DPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPG mean reward : -195.48952623813094\n",
      "DPG std reward : 559.100359501424\n",
      "DPG best reward : -0.2882373673013595\n",
      "DPG # under 1: 3\n",
      "DPG # under 5: 4\n"
     ]
    }
   ],
   "source": [
    "dpgStats = resultsTable('DPG', dpgData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MBACD\n",
    "\n",
    "In order to take advantage of a system model and the reference trajectory, our algorithm uses different inputs for the policy and Q function.\n",
    "\n",
    "### System model\n",
    "\n",
    "$$g(\\underline x_k, \\underline u_k) \\sim GP(\\mu([\\underline x_k, \\underline u_k]^T), k([\\underline x_k, \\underline u_k]^T, [\\underline x, \\underline u_k]^T)) $$\n",
    "\n",
    "### Policy function\n",
    "\n",
    "The policy function takes the following form.\n",
    "\n",
    "$$\\pi_\\theta( \\underline u \\ |\\  \\underline x, \\underline r) = \\mathcal{N}(\\mu_\\theta(\\underline z), \\sigma_\\theta(\\underline z))$$\n",
    "\n",
    "Again we use a stochastic policy, a Normal distribution parametrised by $\\mu(\\underline z)$ and $\\sigma (\\underline z)$, with $\\underline z_k = [r_{k+1} - x_k, ..., r_{k+h} - x_k]$. Doing this has the benefit of reducing the range of input dimensions and centering it around zero, which is similar to the input normalization used in many deep learning applicaitons. It also puts an enfasis on the change is has to make to reach the next reference, instead of on the current absolute state, which might help it generalize to unknow states. Another benefit is the reduction of the number of inputs, reducing it by one, reducing the number of parameters of the network.\n",
    "\n",
    "The objective policy gradient takes the following form.\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\sum^{D} \\omega \\cdot A \\cdot \\nabla_\\theta \\ln \\pi(\\underline u | \\underline z) $$\n",
    "\n",
    "In this case the advantage function is the only the Q-Function.\n",
    "\n",
    "### Q-Function\n",
    "\n",
    "The Q function is also enhanced with more input information. Similar to the inputs of the policy, we use the difference from the reference to the future predicted states. \n",
    "\n",
    "$$ \\underline \\epsilon_{k+1} = \\underline r_{k+1} - \\underline x_{k+1} $$\n",
    "$$ \\underline \\epsilon_{k+1} = \\underline r_{k+1} - g(\\underline x_k, \\underline u_k)$$\n",
    "\n",
    "The Q function being an approximation of the future expected rewards of the policy, it makes sense that by providing the projected future deviations, it could make a better judgment on the quality of the current state-action pair. This is only sensible in a discounted reward scenario, were the future rewards influence the current Q value. \n",
    "\n",
    "$$ \\underline d = \\left [\\underline u_k^T, \\underline \\epsilon_{k}^T, \\underline \\epsilon_{k+1}^T, ..., \\underline \\epsilon_{k+h}^T, \\right ]^T $$\n",
    "\n",
    "$$ Q_\\phi (\\underline d) $$\n",
    "\n",
    "The reason the advantage function is only the Q-Function is that to calculate the value function in a similar fashion as on the AC Algorithm, one would need to calculate multiple future actions and states depending on the amount of samples drawn to aproximate the expected value based on the initial action. This would slow down training, and I belive is not the main problem this algorithm has right now.\n",
    "\n",
    "### Training\n",
    "\n",
    "Excluding the inputs of both functions, the training is equal to the Actor Critic algorithm from before. (Importance sampling, replay buffers, target networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisys\n",
    "**Pros (allegedly)**\n",
    " - Policy and Q-Function requiere only distant from reference (better generalization in unknown states)\n",
    " - Usage of predicted future error, to make better predictions of expected discounted reward\n",
    " - Uses the predictions in the input instead of directly backpropagating through them (model predictive control/imagination rollouts) (less operations, avoids vanishing/exploding gradient)\n",
    "\n",
    "**Cons**\n",
    " - Requieres system model\n",
    " - Extra operations during training to predict h future states\n",
    " - Only usable with system references\n",
    " - If reference doesn't cover full state, leads to extremely ugly code and formulas\n",
    " \n",
    "Clearly the critic is not learning\n",
    " \n",
    "**Why?**\n",
    " \n",
    " - Error in code\n",
    " - Best reward when $Q(u_k, 0, 0, 0)$, which might be the reson is not converging\n",
    " - Network might be to small to, needs to be bigger for the extra inputs\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710384d8182149398d0ecd79351559ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mbacdData = importExperimentData('MBACD')\n",
    "mbacdMean = plotExpData(mbacdData, 'MBACD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBACD mean reward : -1760.5598051685724\n",
      "MBACD std reward : 2543.801269633103\n",
      "MBACD best reward : -64.47958622468511\n",
      "MBACD # under 1: 0\n",
      "MBACD # under 5: 0\n"
     ]
    }
   ],
   "source": [
    "mbacdStats = resultsTable('MBACD', mbacdData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>best</th>\n",
       "      <th>under1</th>\n",
       "      <th>under5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REINFORCE</td>\n",
       "      <td>-455.788509</td>\n",
       "      <td>1156.093730</td>\n",
       "      <td>-0.221563</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AC</td>\n",
       "      <td>-232.307507</td>\n",
       "      <td>558.821281</td>\n",
       "      <td>-25.013296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DPG</td>\n",
       "      <td>-195.489526</td>\n",
       "      <td>559.100360</td>\n",
       "      <td>-0.288237</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MBACD</td>\n",
       "      <td>-1760.559805</td>\n",
       "      <td>2543.801270</td>\n",
       "      <td>-64.479586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name         mean          std       best  under1  under5\n",
       "0  REINFORCE  -455.788509  1156.093730  -0.221563       2       3\n",
       "1         AC  -232.307507   558.821281 -25.013296       0       0\n",
       "2        DPG  -195.489526   559.100360  -0.288237       3       4\n",
       "3      MBACD -1760.559805  2543.801270 -64.479586       0       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsDF = pd.DataFrame([reinforceStats, acStats, dpgStats, mbacdStats])\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b4327c2c9b4e718aa66ae186965a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.plot(reinforceMean, label='REINFORCE')\n",
    "plt.plot(acMean, label='AC')\n",
    "plt.plot(dpgMean, label='DPG')\n",
    "plt.plot(mbacdMean, label='MBACD')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Q-Function Gradients\n",
    "\n",
    "This method is an attempt of combining the MBACD methods with DPG. The policy and the Q-Function inputs/parametrization are identical to the MBACD algorithm. The improvement (maybe?) relies on the calculating the gradient directly from the Q-Function, similar to DPG.\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = \\sum^{\\mathcal D} \\nabla_{u_k} Q_\\phi(\\underline d) \\nabla_\\theta \\pi_\\theta(\\underline z_k) |_{u_k = \\pi_\\theta} $$\n",
    "\n",
    "This formula would take advantage of the more stable deterministic policy gradient idea of directly maximazing the reward of the Q-Function by backpropagating through it. But, because of the input of the Q-Function depends on almost all its inputs on the selected action $u_k$, the backpropagation is procedure is also different.\n",
    "\n",
    "$$ \\frac{\\partial Q_\\phi (\\underline d)}{\\partial u_k} = \\frac{\\partial Q_\\phi (\\underline d)}{\\partial \\underline d} \\cdot \\frac{\\partial \\underline d}{\\partial u_k} $$\n",
    "\n",
    "The first part of the operation is the same backprogation through the network, which is standart in any Deep Learning/Neural Network and doesn't require much explanation. \n",
    "\n",
    "$$ \\frac{\\partial \\underline d}{\\partial u_k} = \\left [ 1, 0, -\\frac{\\partial g(\\underline x_{k}, \\underline u_{k})}{\\partial \\underline u_k},  -\\frac{\\partial g(\\underline x_{k+1}, \\underline u_{k+1})}{\\partial \\underline x_{k+1}} \\cdot \\frac{\\partial g(\\underline x_{k}, \\underline u_{k})}{\\partial \\underline u_k}, ...\\right ]$$\n",
    "\n",
    "The gradient of this Q-Function takes not only into account the effects the policy has based on the current state (and indirectly on the discounted future rewards). Here, the future decisions that will be taken by the policy are also taken into account for the update. This steps puts a requirement on the system model, as it should be differentiable to backpropagate to it. GP are ideal for that reason. \n",
    "\n",
    "Idealy, because the future rewards are discounted, the effect of those extra gradient steps would be proportional. Some fine tuning of the discount factor might be necessary, or applying it directly in the gradient process. \n",
    "\n",
    "It also depends on the Q-Function learning, which is not the case :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TRLFPI]",
   "language": "python",
   "name": "conda-env-TRLFPI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
